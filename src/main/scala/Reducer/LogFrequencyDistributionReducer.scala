package com.laxmena.Reducer

import org.apache.hadoop.io.{IntWritable, Text}
import org.apache.hadoop.mapreduce.Reducer

import java.lang.Iterable
import scala.collection.JavaConverters.*

/**
 * <b>LogFrequencyDistributionReducer</b>: Aggregates the output from LogFrequencyDistributionMapper to generate
 * Distribution of Log Levels for different Time Intervals.
 *
 */
class LogFrequencyDistributionReducer  extends Reducer[Text, IntWritable, Text, IntWritable] {
  /**
   * Reducer implementation: Counts the number of LogLevels for each time interval.
   * 
   * @param key
   * @param values
   * @param context
   */
  override def reduce(key: Text, values: Iterable[IntWritable],
                      context: Reducer[Text, IntWritable, Text, IntWritable]#Context): Unit = {
    // Aggregates key value pairs generated by Mapper to generate the distribution of the Logs.
    val sum = values.asScala.foldLeft(0)(_ + _.get)
    context.write(key, new IntWritable(sum))
  }
}
